{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import laspy\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import util_las as las\n",
    "import pathlib\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is intenteded to produce the change detections in the desired file format. Run the first cells to load the proper .csv file and then the cells for the format you want to save the detections to (.las, subset of points to .las, shapefile or .ply mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = 'C:/Users/gwena/Documents/STDL/2_En_cours/qalidar/02_Data'\n",
    "\n",
    "# Output directory\n",
    "folder_dir = 'out_dataframe/criticity_changes_df/'\n",
    "csv_file_name = '2546500_1212000_150_1801-1741.csv'\n",
    "out_dir = 'out_vis/'\n",
    "\n",
    "tile_decript_name = csv_file_name.split('.')[0]\n",
    "vox_dimension = float(tile_decript_name.rsplit('_', maxsplit=2)[1])/100\n",
    "\n",
    "os.chdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder_path = os.path.join(out_dir,tile_decript_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path for the folder and subfolder to store the outgoing file in case it doesn't yet exist\n",
    "pathlib.Path(subfolder_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(folder_dir, csv_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a subset of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = df.groupby('change_criticity_label').apply(lambda x: x.sample(8, random_state=42).reset_index(drop=True)).reset_index(drop=True)\n",
    "\n",
    "subset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to .las file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_file = las.df_to_las(df)\n",
    "\n",
    "las_file.write(os.path.join(subfolder_path, 'change_detection.las'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save subset of the changes detection to .las and to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_file = las.df_to_las(subset_df, index_to_point_source_id=True)\n",
    "\n",
    "las_file.write(os.path.join(subfolder_path, 'subset_change_detections.las'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.to_csv(os.path.join(subfolder_path, 'subset_change_detections.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clustering for isolated voxels removal -> to .las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_df = df[df.change_criticity=='problematic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = problematic_df[['X_grid','Y_grid','Z_grid']]\n",
    "clustering = DBSCAN(eps=1.5, min_samples=2).fit(X)\n",
    "isolated_voxel_mask = (clustering.labels_ == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_criticity=df['change_criticity_label'].values.copy()\n",
    "\n",
    "# Change all voxels that are problematic but isolated to label 14\n",
    "labels_criticity[problematic_df.index[isolated_voxel_mask]] = 14\n",
    "df.loc[:, 'filtered_change_criticity_label'] = labels_criticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_file = las.df_to_las(df, user_data_col='filtered_change_criticity_label')\n",
    "\n",
    "las_file.write(os.path.join(subfolder_path, 'change_detection_filtered.las'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save subset to geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xyz) for xyz in zip(subset_df.X_grid, subset_df.Y_grid, subset_df.Z_grid)]\n",
    "gdf = gpd.GeoDataFrame(subset_df, crs='EPSG:2056', geometry=geometry)\n",
    "gdf['geometry'] = gdf.geometry #.buffer(vox_dimension/2, cap_style=3) # Uncomment to have the shape representing the dimension of the voxel\n",
    "#Remove unnecessary columns\n",
    "gdf.drop(columns=['X_grid','Y_grid','Z_grid','cosine_similarity','second_cosine_similarity','third_cosine_similarity','majority_class'],inplace=True)\n",
    "gdf = gdf.rename_axis('vox_id').reset_index()\n",
    "gdf['validity']=1\n",
    "gdf['comment']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(os.path.join(subfolder_path,'subset_change_detections.gpkg'), drive='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to shapefile\n",
    "Not used currently, partially old code, left for legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_and_grey_df = df[df['change_criticity']!='non_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_and_grey_df.loc[:,'Z_grid'] = problematic_and_grey_df['Z_grid'].astype(str)\n",
    "problematic_and_grey_df.loc[:,'change_criticity_label'] = problematic_and_grey_df['change_criticity_label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_and_grey_df.loc[:,'vertical_descript'] = problematic_and_grey_df[['Z_grid', 'change_criticity', 'change_criticity_label']].agg('-'.join, axis=1)\n",
    "problematic_and_grey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_df = problematic_and_grey_df[problematic_and_grey_df['change_criticity']=='problematic']\n",
    "grey_zone_df = problematic_and_grey_df[problematic_and_grey_df['change_criticity']=='grey_zone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspired from https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings\n",
    "# We want to group all the voxels having the same planar coordinates, and create a string with every vertical descript\n",
    "def f(x):\n",
    "    return pd.Series(dict( vertical_descript = \"{%s}\" % '\\n'.join(x['vertical_descript'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_problematic_df = problematic_df.groupby(['X_grid','Y_grid']).apply(f).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_grey_zone_df = grey_zone_df.groupby(['X_grid','Y_grid']).apply(f).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(grouped_problematic_df.X_grid, grouped_problematic_df.Y_grid)]\n",
    "gdf_problematic = gpd.GeoDataFrame(grouped_problematic_df, crs='EPSG:2056',geometry=geometry)\n",
    "gdf_problematic['geometry'] = gdf_problematic.geometry.buffer(voxel_dimension/2, cap_style=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(grouped_grey_zone_df.X_grid, grouped_grey_zone_df.Y_grid)]\n",
    "gdf_grey_zone = gpd.GeoDataFrame(grouped_grey_zone_df, crs='EPSG:2056',geometry=geometry)\n",
    "gdf_grey_zone['geometry'] = gdf_grey_zone.geometry.buffer(voxel_dimension/2, cap_style=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without concatenating the shapes\n",
    "geometry = [Point(xyz) for xyz in zip(problematic_df.X_grid, problematic_df.Y_grid, problematic_df.Z_grid)]\n",
    "gdf_complete_problematic = gpd.GeoDataFrame(problematic_df[['X_grid','Y_grid','Z_grid','change_criticity','change_criticity_label','vertical_descript']], crs='EPSG:2056', geometry=geometry)\n",
    "gdf_complete_problematic['valid_detection']=1\n",
    "gdf_complete_problematic['geometry'] = gdf_complete_problematic.geometry.buffer(voxel_dimension/2, cap_style=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xyz) for xyz in zip(grey_zone_df.X_grid, grey_zone_df.Y_grid, grey_zone_df.Z_grid)]\n",
    "gdf_complete_grey_zone = gpd.GeoDataFrame(grey_zone_df[['X_grid','Y_grid','Z_grid','change_criticity','change_criticity_label','vertical_descript']], crs='EPSG:2056',geometry=geometry)\n",
    "gdf_complete_grey_zone['valid_detection']=1\n",
    "gdf_complete_grey_zone['geometry'] = gdf_complete_grey_zone.geometry.buffer(voxel_dimension/2, cap_style=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_complete_problematic.to_file(f'/mnt/data-01/nmunger/out_shapefile/problematic_without_concat_{tile_name}_{voxel_dimension}.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_complete_grey_zone.to_file(f'/mnt/data-01/nmunger/out_shapefile/grey_zone_without_concat_{tile_name}_{voxel_dimension}.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_problematic.to_file(f'/mnt/data-01/nmunger/out_shapefile/problematic_{tile_name}_{voxel_dimension}.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_grey_zone.to_file(f'/mnt/data-01/nmunger/out_shapefile/grey_zone_{tile_name}_{voxel_dimension}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to voxel mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_voxels_mesh(voxels_center, voxel_width, voxel_height):\n",
    "    '''voxels_center: dataframe of size nx3, X|Y|Z '''\n",
    "    for i in range(len(voxels_center)):\n",
    "        center = voxels_center.iloc[i,:3].to_numpy()\n",
    "        \n",
    "        # Create the cube mesh\n",
    "        cube_mesh = o3d.geometry.TriangleMesh.create_box(width=voxel_width, height=voxel_height, depth=voxel_width)\n",
    "\n",
    "        # Translate the cube to the desired center point\n",
    "        cube_mesh.translate(center-np.array([voxel_width/2, voxel_width/2, voxel_height/2]))\n",
    "\n",
    "        if i == 0: # If generating first voxel mesh\n",
    "            total_mesh=cube_mesh\n",
    "            continue\n",
    "        else:\n",
    "            total_mesh+=cube_mesh\n",
    "    \n",
    "    return total_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for change_type in df.change_criticity.unique():\n",
    "    voxel_mesh = generate_voxels_mesh( df.loc[df['change_criticity']==change_type, ['X_grid','Y_grid','Z_grid']], vox_dimension, vox_dimension)\n",
    "    o3d.io.write_triangle_mesh(os.path.join(subfolder_path,f'{change_type}.ply'), voxel_mesh, write_vertex_colors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qalidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
