{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import util_las as las\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the previous and the new point cloud\n",
    "WORKING_DIR = 'C:/Users/gwena/Documents/STDL/2_En_cours/qalidar/02_Data'\n",
    "\n",
    "prev_path = '2018_NE_retiled/2546500_1212000.las'\n",
    "new_path = '2022_Neuchatel/2546500_1212000.laz'\n",
    "tile_name = os.path.basename(new_path).split('.')[0] # Is used for the naming of the .csv outgoing file\n",
    "\n",
    "classes_correspondance_path = 'classes_equivalence.csv' \n",
    "vox_xy = 1.5 # Voxel size in meters\n",
    "vox_z = 1.5\n",
    "\n",
    "os.chdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_pc_df = las.las_to_df_xyzclass(prev_path)\n",
    "new_pc_df = las.las_to_df_xyzclass(new_path)\n",
    "\n",
    "# Remove all points which are noise in the previous generation as they do not bring useful information\n",
    "prev_pc_df = prev_pc_df[prev_pc_df['classification']!=7]\n",
    "\n",
    "# Match the supplementary class to classes from the previous generation\n",
    "new_pc_df = las.reclassify(new_pc_df, classes_correspondance_path) \n",
    "\n",
    "# Set the lowest coordinates of the point clouds in each axis as the origin of the common grid \n",
    "x_origin = min(prev_pc_df.X.min(), new_pc_df.X.min())\n",
    "y_origin = min(prev_pc_df.Y.min(), new_pc_df.Y.min())\n",
    "z_origin = min(prev_pc_df.Z.min(), new_pc_df.Z.min())\n",
    "# Same logic for the highest coordinates\n",
    "x_max = max(prev_pc_df.X.max(), new_pc_df.X.max())\n",
    "y_max = max(prev_pc_df.Y.max(), new_pc_df.Y.max())\n",
    "z_max = max(prev_pc_df.Z.max(), new_pc_df.Z.max())\n",
    "\n",
    "grid_origin = x_origin, y_origin, z_origin\n",
    "\n",
    "grid_max = x_max, y_max, z_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_voxelised_df = las.to_voxelised_df(prev_pc_df, grid_origin, grid_max, vox_xy, vox_z)\n",
    "new_voxelised_df = las.to_voxelised_df(new_pc_df, grid_origin, grid_max, vox_xy, vox_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_columns(df1, df2):\n",
    "    # Modifiy the dataframes if one column is missing compared to the other. If it is the case it adds an empty column\n",
    "    \n",
    "    df1 = df1.copy(deep=True) # Do the modification on a copy of the dataframe\n",
    "    df2 = df2.copy(deep=True)\n",
    "\n",
    "    missing_columns_df1 = set(df2.columns) - set(df1.columns)\n",
    "\n",
    "    for column in missing_columns_df1:\n",
    "        df1[column] = pd.Series(dtype=df2[column].dtype)\n",
    "\n",
    "    missing_columns_df2 = set(df1.columns) - set(df2.columns)\n",
    "\n",
    "    for column in missing_columns_df2:\n",
    "        df2[column] = pd.Series(dtype=df1[column].dtype)\n",
    "\n",
    "    # Make sure that the order of the classification columns is sorted\n",
    "    sorted_class_columns1 = df1.iloc[:,3:].reindex(sorted(df1.iloc[:,3:].columns), axis=1)\n",
    "    df1.drop(df1.columns[3:], axis=1, inplace=True)\n",
    "    df1 = pd.concat([df1, sorted_class_columns1],axis=1)\n",
    "\n",
    "    sorted_class_columns2 = df2.iloc[:,3:].reindex(sorted(df2.iloc[:,3:].columns), axis=1)\n",
    "    df2.drop(df2.columns[3:], axis=1, inplace=True)\n",
    "    df2 = pd.concat([df2, sorted_class_columns2],axis=1)\n",
    "\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one class is missing in either of the dataframe compared to the other, create new empty column\n",
    "prev_voxelised_df, new_voxelised_df = align_columns(prev_voxelised_df, new_voxelised_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(prev_voxelised_df.head(2))\n",
    "display(new_voxelised_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up space\n",
    "del prev_pc_df\n",
    "del new_pc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = prev_voxelised_df.merge(new_voxelised_df, on=['X_grid','Y_grid','Z_grid'], how='outer', suffixes=('_prev','_new'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.replace(np.NaN, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path for the folder to store the .csv file in case it doesn't yet exist\n",
    "pathlib.Path('out_dataframe/voxelised_comparison').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# In file name, set voxel size in centimeters, so as to avoid decimal (.) presence in the file name\n",
    "merged_df.to_csv(f'out_dataframe/voxelised_comparison/{tile_name}_{int(vox_xy*100)}-{int(vox_z*100)}'+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sankey Diagram visualisation:\n",
    "Note: the code is awful, not a priority, but might be good to go over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newer_pc= las.las_to_df_xyzclass(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_per_class = newer_pc.groupby('classification')['classification'].count().to_frame('nb_points').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_equivalences = pd.read_csv(classes_correspondance_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_class_name = pd.DataFrame({'class_name':['Unclassified (1)','Ground (2)','Vegetation (3)','Buildings (6)','Noise (7)','Water (9)','Brigdes(17)'], 'id':[101,102,103,106,107,109,117]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sancay = class_equivalences.merge(points_per_class, how='inner',left_on='id',right_on='classification')\n",
    "df_sancay.matched_id=df_sancay.matched_id+100 # Simply to attribute other labels\n",
    "df_sancay.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = pd.concat([df_sancay[['id','class_name']],matched_class_name]).reset_index(drop=True)\n",
    "all_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sancay.merge(all_nodes.reset_index(),left_on='matched_id', right_on='id', how='inner').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sancay.merge(all_nodes.reset_index(),left_on='matched_id', right_on='id', how='inner').sort_values(by='id_x')['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sancay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = all_nodes.class_name,#[\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"],\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = np.arange(0,18), # indices correspond to labels, eg A1, A2, A1, B1, ...\n",
    "      target = df_sancay.merge(all_nodes.reset_index(),left_on='matched_id', right_on='id', how='inner').sort_values(by='id_x')['index'],\n",
    "      value = df_sancay.nb_points\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text=\"Basic Sankey Diagram\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qalidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
